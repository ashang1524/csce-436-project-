<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, viewport-fit=cover">
    <title>Grocery Helper</title>
    <meta name="description" content="CSCE 436 Project Webpage">
    <link href="source-code-pro.css" rel="stylesheet">
    <link href="styles.css" rel="stylesheet">
</head>

<body>
    <h1>Grocery Helper</h1>
    Jack Sebastian, Andrew Shang, James Garcia, Luis Martinez
    Morales, and Akhil Mathew


    <div class="section1">
        <h2>Introduction and Motivation</h2>
        <p>
            4 Corners of Earth is a company created by Jack Sebastian, Andrew Shang, James Garcia, Luis
            Martinez
            Morales, and Akhil Mathew. The main product we are launching is the Grocery Helper. In the
            US there are
            currently approximately 1.3 million legally blind people, and over 12 million Americans over
            the age of
            40
            who deal with some sort of visual impairment. Taking this in mind, we set out to help
            Americans regain
            some
            of their vision back. Additionally, we believe this product would be useful for people that
            have
            traveled to
            a new country and need to identify grocery items. It is important to note that currently 4
            out of our 5
            founding members have to rely on the use of glasses or contact lenses to see clearly. We
            know what it's
            like
            to have mild visual impairments and that through age our vision can get worse. We truly
            believe in our
            goal
            to aid people who suffer from worse visual impairments. More specifically, our product
            creates bounding
            boxes on any object and uses a neural network trained on a variety of items. We also have a
            feature that
            can
            read the name of the object in case that viewer needs auditory help. Currently the software
            we created
            works
            online but our goal is to rolle it out to mobile devices and potentially glasses with a
            screen.

        </p>
    </div>
    <div id="col">
        <h2>YouTube Promotion Video</h2>
        <iframe width="560" height="315" src="https://www.youtube.com/embed/pPE5jZwpf2I" title="YouTube video player"
            frameborder="0"
            allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
            allowfullscreen></iframe>

        <h2>Presentation Slides</h2>
        <iframe
            src="https://docs.google.com/presentation/d/e/2PACX-1vR0uppzxORn2uXEgSgbaKmtbLUv1aCTO9gzI2iSBfviQSldaCHI95VmbHlFhj8MtfO9Tzw4BtNf9et0/embed?start=false&loop=false&delayms=3000"
            frameborder="0" width="760" height="569" allowfullscreen="true" mozallowfullscreen="true"
            webkitallowfullscreen="true"></iframe>
    </div>

    <div class="insights">
        <h2>Key Insights from Peer Feedback</h2>
        <ul style="padding-left: 20px;">
            <li style="font-size: large;">“Curious to know how it would handle multiple objects at once. Would it say
                all of them? Would it switch
                between them, if so, how?”
                <ul style="padding-left: 60px;">
                    <li style="font-size: medium;">Currently our product handles multiple objects at once by speaking
                        aloud the name of the last
                        object that was detected. This can be inefficient if you are trying to look for a specific item
                        so in a later implementation, we plan on adding a feature that will let you pick a specific item
                        you are looking for. This way, when that item is detected among multiple objects, only that item
                        will be read aloud.</li>
                </ul>
            </li>

            <li style="font-size: large;">“I think it would be cool to improve the UI a bit and add the ability to find
                the different types of
                chips.”
                <ul style="padding-left: 60px;">
                    <li style="font-size: medium;">Our user implementation definitely needs some improvement. Right now
                        we just have some basic
                        object detection but in the future, we plan on training our model even further so that it can
                        detect more items and it can detect more specific items such as different brands of chips or
                        different types of canned food.</li>
                </ul>
            </li>

            <li style="font-size: large;">“I liked that you pivoted quickly from the glasses to something more portable
                that people can quickly
                and easily carry around. It's a novel idea that has a lot of real world use!”
                <ul style="padding-left: 60px;">
                    <li style="font-size: medium;">Our first thought even in the beginning stages of making this product
                        was the users. We wanted
                        the users to enjoy using our product and not be frustrated with a difficult to use product. That
                        is why we made the switch out of smart glasses because not everyone has glasses or wants to wear
                        one, but a lot more people have access to the internet.</li>
                </ul>
            </li>

            <li style="font-size: large;">“I like how they made simplicity a priority in their project which I think
                will help a lot of users.”
                <ul style="padding-left: 60px;">
                    <li style="font-size: medium;">We wanted our users to have an easy time using our product. That is
                        why we did everything we
                        could to try and simplify our product. We did this by adding instructions and having very few
                        buttons. Our product is intuitive and easy to use.</li>
                </ul>
            </li>
        </ul>
    </div>

    <div class="conclusion">
        <h2>Conclusion and Future Work</h2>
        Through our testing, we found that a person with severe vision impairment is able to detect objects if given
        proper audio support. We also found that our product is able to provide this support within a limited setting.
        While for the project we focused on a grocery setting, by expanding the model we used for our object detection
        there could be a whole host of locations that our product would help the visually impaired go to. For example
        other types of stores or maybe even just walking down the street without a walking stick. Our current product
        state is a desktop app, but a future improvement we could make would be making it into an app or even usable on
        products like the Google Glasses to make it much more mobile. One big problem our product faced was detecting
        too many objects as it would detect any object that it would see. Another problem was it would have trouble
        detecting objects far away. Thus, fixing both of these issues would be a massive improvement to our project.
        Lastly, our project couldn't read words on packaging and would classify different things in the same packaging
        as the same thing. To improve upon this we could add some form of word detection to our project or possibly a
        barcode scanner. Both of these improvements could also help in reading ingredients on packaging, which is known
        to be in small print and difficult for those with vision impairment. If all these improvements were to be made
        our product as shown through our needs finding study could make a real difference in the life of those with
        severe seeing impairment.

    </div>


</body>

</html>